---
title: Pathologies in hierarchical models 
title-slide-attributes:
    data-background-image: "https://qfcatmsu.github.io/Images/dark-big.png"  
    data-background-size: "40%"
    data-background-position: "6% 95%"
subtitle: FW 891 <br> [Click here to view presentation online](https://qfcatmsu.github.io/FW-Bayes/week6/lec9.html){style="position:absolute;top:40%;left:35%;font-size:20px;"}
author: Christopher Cahill
date: 16 October 2023
date-format: "D MMMM YYYY"
format: 
  revealjs:
    css: "https://qfcatmsu.github.io/css/presStyle.css"
    slide-number: c/t  
    theme: simple 
editor: visual
highlight-style: kate
---

## Purpose

-   Introduce some background and theoretical concepts
-   The Devil's (aka Neal's) funnel (spooky seaz'n ðŸ‘»)
-   Examine some ways to track down where things are going wrong in a model
-   Look at some more complicated examples
    -   Examples of how this stuff manifests in real-world models

## Background

-   Many of the most exciting problems in applied statistical ecology involve intricate, high-dimensional models, and sparse data (at least relative to model complexity)

[Betancourt and Girolami 2013]{.footerRight}

## Background

-   Many of the most exciting problems in applied statistical ecology involve intricate, high-dimensional models, and sparse data (at least relative to model complexity)

-   In situations where the data alone cannot identify a model, significant prior information is required to draw valid inference


[Betancourt and Girolami 2013]{.footerRight}

## Background

-   Many of the most exciting problems in applied statistical ecology involve intricate, high-dimensional models, and sparse data (at least relative to model complexity)

-   In situations where the data alone cannot identify a model, significant prior information is required to draw valid inference

-   Such prior information is not limited to an explicit prior distribution, but instead can be encoded in the model construction itself ðŸ¤¯

[Betancourt and Girolami 2013]{.footerRight}

## A one level hierarchical model

$$
\pi(\theta, \phi \mid \mathcal{D}) \propto \prod_{i=1}^{n} \pi\left(\mathcal{D}_{i} \mid \theta_{i}\right) \pi\left(\theta_{i} \mid \phi\right) \pi(\phi)
$$

[Betancourt and Girolami 2013]{.footerRight}

## A one level hierarchical model

$$
\pi(\theta, \phi \mid \mathcal{D}) \propto \prod_{i=1}^{n} \pi\left(\mathcal{D}_{i} \mid \theta_{i}\right) \pi\left(\theta_{i} \mid \phi\right) \pi(\phi)
$$

-   Hierarchical models are defined by the organization of a model's parameters into exchangeable groups, and the resulting conditional independencies between those groups

[Betancourt and Girolami 2013]{.footerRight}

## A one level hierarchical model

$$
\pi(\theta, \phi \mid \mathcal{D}) \propto \prod_{i=1}^{n} \pi\left(\mathcal{D}_{i} \mid \theta_{i}\right) \pi\left(\theta_{i} \mid \phi\right) \pi(\phi)
$$

-   Hierarchical models are defined by the organization of a model's parameters into exchangeable groups, and the resulting conditional independencies between those groups

-   Can also visualize this as a directed acyclic graph (DAG)

[Betancourt and Girolami 2013]{.footerRight}

## Hierarchical DAG

![](images/DAG.png){fig-align="center" width="800" height="600"}

[Betancourt and Girolami 2013]{.footerRight}

## A one-level hierarchical model

$$
\begin{aligned}
y_{i} & \sim {N}\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim {N}\left(\mu, \tau^{2}\right), \text { for } i=1, \ldots, I
\end{aligned}
$$

[Betancourt and Girolami 2013]{.footerRight}

## A one-level hierarchical model

$$
\begin{aligned}
y_{i} & \sim {N}\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim {N}\left(\mu, \tau^{2}\right), \text { for } i=1, \ldots, I
\end{aligned}
$$

-   In terms of the previous equations, $\mathcal{D}=\left(y_{i}, \sigma_{i}\right), \phi=(\mu, \tau), \text { and } \theta=\left(\theta_{i}\right)$

[Betancourt and Girolami 2013]{.footerRight}

## A one-level hierarchical model

$$
\begin{aligned}
y_{i} & \sim {N}\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim {N}\left(\mu, \tau^{2}\right), \text { for } i=1, \ldots, I
\end{aligned}
$$

-   In terms of the previous equations, $\mathcal{D}=\left(y_{i} \sigma_{i}\right), \phi=(\mu, \tau), \text { and } \theta=\left(\theta_{i}\right)$
-   Call any elements of $\phi$ *global* parameters

[Betancourt and Girolami 2013]{.footerRight}

## A one-level hierarchical model

$$
\begin{aligned}
y_{i} & \sim {N}\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim {N}\left(\mu, \tau^{2}\right), \text { for } i=1, \ldots, I
\end{aligned}
$$

-   In terms of the previous equations, $\mathcal{D}=\left(y_{i}, \sigma_{i}\right), \phi=(\mu, \tau), \text { and } \theta=\left(\theta_{i}\right)$
-   Call any elements of $\phi$ *global* parameters
-   Call any elements of $\theta$ *local* parameters

[Betancourt and Girolami 2013]{.footerRight}

## A one-level hierarchical model

$$
\begin{aligned}
y_{i} & \sim {N}\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim {N}\left(\mu, \tau^{2}\right), \text { for } i=1, \ldots, I
\end{aligned}
$$

-   In terms of the previous equations, $\mathcal{D}=\left(y_{i} \sigma_{i}\right), \phi=(\mu, \tau), \text { and } \theta=\left(\theta_{i}\right)$
-   Call any elements of $\phi$ *global* parameters
-   Call any elements of $\theta$ *local* parameters
-   However, recognize this nomencalture breaks down in situations with more levels

[Betancourt and Girolami 2013]{.footerRight}

## A key pathology

-   Unfortunately, this one-level model exhibits some of the typical pathologies of hierarchical models
-   Small changes in $\phi$ induce large changes in density
-   When data are sparse, the density of these models looks like a "funnel"
    -   Region of high density but low volume, and a region of low density but high volume
-   However, the probability mass of these two regions is the same (or nearly so)
-   Any algorithm must be able to manage the dramatic variations in curvature to fully map out the posterior

[Betancourt and Girolami 2013]{.footerRight}

## Naive model implementations

-   Assuming a normal model with no data, a latent mean $\mu$ set at zero, and a lognormal prior on the variance $\tau^{2}=e^{v^{2}}$

[Betancourt and Girolami 2013]{.footerRight}

## Naive model implementations

-   Assuming a normal model with no data, a latent mean $\mu$ set at zero, and a lognormal prior on the variance $\tau^{2}=e^{v^{2}}$

$$
\pi\left(\theta_{1}, \ldots, \theta_{n}, v\right) \propto \prod_{i=1}^{n} N\left(x_{i} \mid 0,\left(e^{-v / 2}\right)^{2}\right) N\left(v \mid 0,3^{2}\right)
$$

[Betancourt and Girolami 2013]{.footerRight}

## Naive model implementations

-   Assuming a normal model with no data, a latent mean $\mu$ set at zero, and a lognormal prior on the variance $\tau^{2}=e^{v^{2}}$

$$
\pi\left(\theta_{1}, \ldots, \theta_{n}, v\right) \propto \prod_{i=1}^{n} N\left(x_{i} \mid 0,\left(e^{-v / 2}\right)^{2}\right) N\left(v \mid 0,3^{2}\right)
$$

-   This hierarchical structure induces large correlations between $v$ and each $\theta_{i}$

[Betancourt and Girolami 2013]{.footerRight}

## Visualizing the pathology ðŸ˜ˆ

![](images/funnel.png){fig-align="center" width="800" height="600"}

## Some things worth noting

-   There is position dependence in the correlation structure, i.e., correlation changes depending on where you are located in the posterior

[Betancourt and Girolami 2013]{.footerRight}

## Some things worth noting

-   There is position dependence in the correlation structure, i.e., correlation changes depending on where you are located in the posterior
-   No global correction, like rotating or rescaling will solve this problem!

[Betancourt and Girolami 2013]{.footerRight}

## Some things worth noting

-   There is position dependence in the correlation structure, i.e., correlation changes depending on where you are located in the posterior
-   No global correction, like rotating or rescaling will solve this problem!
-   Often manifests as a divergent transition in Stan, as HMC cannot accurately explore the posterior

[Betancourt and Girolami 2013]{.footerRight}

## How can we fix this problem?

-   Remember that the prior information we include in an analysis is not only limited to the choice of an explict prior distribution

[Betancourt and Girolami 2013]{.footerRight}

## How can we fix this problem?

-   Remember that the prior information we include in an analysis is not only limited to the choice of an explict prior distribution
-  The dependence between layers in our model can actually be broken up by reparameterizing the existing parameters into a so-called "non-centered" parameterization
    - Think about the DAG
[Betancourt and Girolami 2013]{.footerRight}

## How can we fix this problem?

-   Remember that the prior information we include in an analysis is not only limited to the choice of an explict prior distribution
-  The dependence between layers in our model can actually be broken up by reparameterizing the existing parameters into a so-called "non-centered" parameterization
    - Think about the DAG
-  Non-centered parameterizations factor certain dependencies into deterministic
transformations between the layers, leaving the actively sampled variables uncorrelated

[Betancourt and Girolami 2013]{.footerRight}

## Centered vs. non-centered model maths 

::: {style="font-size: 85%;"}

Centered model:
$$
\begin{aligned}
y_{i} & \sim {N}\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim {N}\left(\mu, \tau^{2}\right), \end{aligned}
$$

Non-centered analog:

:::

[Betancourt and Girolami 2013]{.footerRight}

## Centered vs. non-centered model maths 

::: {style="font-size: 85%;"}

Centered model:
$$
\begin{aligned}
y_{i} & \sim {N}\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim {N}\left(\mu, \tau^{2}\right), \end{aligned}
$$

Non-centered analog:

$$
\begin{aligned}
y_{i} & \sim N\left(\vartheta_{i} \tau+\mu, \sigma_{i}^{2}\right) \\
\vartheta_{i} & \sim N(0,1).
\end{aligned}
$$


:::

[Betancourt and Girolami 2013]{.footerRight}

## Centered vs. non-centered model maths 

::: {style="font-size: 85%;"}

Centered model:
$$
\begin{aligned}
y_{i} & \sim {N}\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim {N}\left(\mu, \tau^{2}\right), \end{aligned}
$$

Non-centered analog:

$$
\begin{aligned}
y_{i} & \sim N\left(\vartheta_{i} \tau+\mu, \sigma_{i}^{2}\right) \\
\vartheta_{i} & \sim N(0,1).
\end{aligned}
$$

Key point: NCP shifts correlation from the latent parameters to data 

:::

[Betancourt and Girolami 2013]{.footerRight}

## Centered vs. non-centered model DAG 

![](images/DAG2.png){fig-align="center" width="800" height="500"}

[Betancourt and Girolami 2013]{.footerRight}

## When does NCP help? 

![](images/timing.png){fig-align="center" width="450" height="500"}

[Betancourt and Girolami 2013]{.footerRight}

## Example

![](images/ghosts.jpg){fig-align="center" width="900" height="500"}

## Example 

- consider the one-way normal model with 800 latent $\theta_{i}$
- constant measurement error $\sigma_{i} = \sigma = 10$
- latent parameters are $\mu = 8, \tau = 3$
- $\theta_{i}$ and $y_{i}$ sampled randomly 

<br>

[Betancourt and Girolami 2013]{.footerRight}

## Example 

- consider the one-way normal model with 800 latent $\theta_{i}$
- constant measurement error $\sigma_{i} = \sigma = 10$
- latent parameters are $\mu = 8, \tau = 3$
- $\theta_{i}$ and $y_{i}$ sampled randomly 

<br>

Add weakly informative priors to this generative likelihood

$$
\begin{array}{l}
\pi(\mu)=N\left(0,5^{2}\right) \\
\pi(\tau)=\text { Half-Cauchy }(0,2.5) .
\end{array}
$$

[Betancourt and Girolami 2013]{.footerRight}

## Example, centered vs. noncentered 

::: {style="font-size: 85%;"}

The centered parameterization of this model can be written as

$$
\begin{aligned}
y_{i} & \sim N\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim N\left(\mu, \tau^{2}\right), \text { for } i=1, \ldots, 800
\end{aligned}
$$

and it should have inferior performance relative to the noncentered model: 

$$
\begin{aligned}
y_{i} & \sim N\left(\tau \vartheta_{i}+\mu, \sigma_{i}^{2}\right) \\
\vartheta_{i} & \sim N(0,1), \text { for } i=1, \ldots, 800
\end{aligned}
$$

:::

[Betancourt and Girolami 2013]{.footerRight}

## Implementing that in code 

::: {style="font-size: 85%;"}

```{javascript echo = T, eval = F}
transformed data {
  real mu;
  real<lower=0> tau;
  real alpha;
  int N;
  mu = 8;
  tau = 3;
  alpha = 10;
  N = 800;
}
generated quantities {
  real mu_print;
  real tau_print;
  vector[N] theta;
  vector[N] sigma;
  vector[N] y;
  mu_print = mu;
  tau_print = tau;
  for (i in 1:N) {
    theta[i] = normal_rng(mu, tau);
    sigma[i] = alpha;
    y[i] = normal_rng(theta[i], sigma[i]);
  }
}
```
:::

## Calling that from R 
