---
title: Pathologies in hierarchical models 
title-slide-attributes:
    data-background-image: "https://qfcatmsu.github.io/Images/dark-big.png"  
    data-background-size: "40%"
    data-background-position: "6% 95%"
subtitle: FW 891 <br> [Click here to view presentation online](https://qfcatmsu.github.io/FW-Bayes/week6/lec9.html){style="position:absolute;top:40%;left:35%;font-size:20px;"}
author: Christopher Cahill
date: 16 October 2023
date-format: "D MMMM YYYY"
format: 
  revealjs:
    css: "https://qfcatmsu.github.io/css/presStyle.css"
    slide-number: c/t  
    theme: simple 
editor: visual
highlight-style: kate
---

## Purpose

-   Introduce some background and theoretical concepts
-   The Devil's funnel (spooky seaz'n ðŸ‘»)
-   What to do about it
-   Example
-   Example #2 (Eight schools)

## Background

-   Many of the most exciting problems in applied statistical ecology involve intricate, high-dimensional models, and sparse data (at least relative to model complexity)

[Betancourt and Girolami 2013]{.footerRight}

## Background

-   Many of the most exciting problems in applied statistical ecology involve intricate, high-dimensional models, and sparse data (at least relative to model complexity)

-   In situations where the data alone cannot identify a model, significant prior information is required to draw valid inference


[Betancourt and Girolami 2013]{.footerRight}

## Background

-   Many of the most exciting problems in applied statistical ecology involve intricate, high-dimensional models, and sparse data (at least relative to model complexity)

-   In situations where the data alone cannot identify a model, significant prior information is required to draw valid inference

-   Such prior information is not limited to an explicit prior distribution, but instead can be encoded in the model construction itself ðŸ¤¯

[Betancourt and Girolami 2013]{.footerRight}

## A one level hierarchical model

$$
\pi(\theta, \phi \mid \mathcal{D}) \propto \prod_{i=1}^{n} \pi\left(\mathcal{D}_{i} \mid \theta_{i}\right) \pi\left(\theta_{i} \mid \phi\right) \pi(\phi)
$$

[Betancourt and Girolami 2013]{.footerRight}

## A one level hierarchical model

$$
\pi(\theta, \phi \mid \mathcal{D}) \propto \prod_{i=1}^{n} \pi\left(\mathcal{D}_{i} \mid \theta_{i}\right) \pi\left(\theta_{i} \mid \phi\right) \pi(\phi)
$$

-   Hierarchical models are defined by the organization of a model's parameters into exchangeable groups, and the resulting conditional independencies between those groups

[Betancourt and Girolami 2013]{.footerRight}

## A one level hierarchical model

$$
\pi(\theta, \phi \mid \mathcal{D}) \propto \prod_{i=1}^{n} \pi\left(\mathcal{D}_{i} \mid \theta_{i}\right) \pi\left(\theta_{i} \mid \phi\right) \pi(\phi)
$$

-   Hierarchical models are defined by the organization of a model's parameters into exchangeable groups, and the resulting conditional independencies between those groups

-   Can also visualize this as a directed acyclic graph (DAG)

[Betancourt and Girolami 2013]{.footerRight}

## Hierarchical DAG

![](images/DAG.png){fig-align="center" width="800" height="600"}

[Betancourt and Girolami 2013]{.footerRight}

## A one-level hierarchical model

$$
\begin{aligned}
y_{i} & \sim {N}\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim {N}\left(\mu, \tau^{2}\right), \text { for } i=1, \ldots, I
\end{aligned}
$$

[Betancourt and Girolami 2013]{.footerRight}

## A one-level hierarchical model

$$
\begin{aligned}
y_{i} & \sim {N}\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim {N}\left(\mu, \tau^{2}\right), \text { for } i=1, \ldots, I
\end{aligned}
$$

-   In terms of the previous equations, $\mathcal{D}=\left(y_{i}, \sigma_{i}\right), \phi=(\mu, \tau), \text { and } \theta=\left(\theta_{i}\right)$

[Betancourt and Girolami 2013]{.footerRight}

## A one-level hierarchical model

$$
\begin{aligned}
y_{i} & \sim {N}\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim {N}\left(\mu, \tau^{2}\right), \text { for } i=1, \ldots, I
\end{aligned}
$$

-   In terms of the previous equations, $\mathcal{D}=\left(y_{i} \sigma_{i}\right), \phi=(\mu, \tau), \text { and } \theta=\left(\theta_{i}\right)$
-   Call any elements of $\phi$ *global* parameters

[Betancourt and Girolami 2013]{.footerRight}

## A one-level hierarchical model

$$
\begin{aligned}
y_{i} & \sim {N}\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim {N}\left(\mu, \tau^{2}\right), \text { for } i=1, \ldots, I
\end{aligned}
$$

-   In terms of the previous equations, $\mathcal{D}=\left(y_{i}, \sigma_{i}\right), \phi=(\mu, \tau), \text { and } \theta=\left(\theta_{i}\right)$
-   Call any elements of $\phi$ *global* parameters
-   Call any elements of $\theta$ *local* parameters

[Betancourt and Girolami 2013]{.footerRight}

## A one-level hierarchical model

$$
\begin{aligned}
y_{i} & \sim {N}\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim {N}\left(\mu, \tau^{2}\right), \text { for } i=1, \ldots, I
\end{aligned}
$$

-   In terms of the previous equations, $\mathcal{D}=\left(y_{i} \sigma_{i}\right), \phi=(\mu, \tau), \text { and } \theta=\left(\theta_{i}\right)$
-   Call any elements of $\phi$ *global* parameters
-   Call any elements of $\theta$ *local* parameters
-   However, recognize this nomencalture breaks down in situations with more levels

[Betancourt and Girolami 2013]{.footerRight}

## A key pathology

-   Unfortunately, this one-level model exhibits some of the typical pathologies of hierarchical models
-   Small changes in $\phi$ induce large changes in density
-   When data are sparse, the density of these models looks like a "funnel"
    -   Region of high density but low volume, and a region of low density but high volume
-   However, the probability mass of these two regions is the same (or nearly so)
-   Any algorithm must be able to manage the dramatic variations in curvature to fully map out the posterior

[Betancourt and Girolami 2013]{.footerRight}

## Naive model implementations

-   Assuming a normal model with no data, a latent mean $\mu$ set at zero, and a lognormal prior on the variance $\tau^{2}=e^{v^{2}}$

[Betancourt and Girolami 2013]{.footerRight}

## Naive model implementations

-   Assuming a normal model with no data, a latent mean $\mu$ set at zero, and a lognormal prior on the variance $\tau^{2}=e^{v^{2}}$

$$
\pi\left(\theta_{1}, \ldots, \theta_{n}, v\right) \propto \prod_{i=1}^{n} N\left(x_{i} \mid 0,\left(e^{-v / 2}\right)^{2}\right) N\left(v \mid 0,3^{2}\right)
$$

[Betancourt and Girolami 2013]{.footerRight}

## Naive model implementations

-   Assuming a normal model with no data, a latent mean $\mu$ set at zero, and a lognormal prior on the variance $\tau^{2}=e^{v^{2}}$

$$
\pi\left(\theta_{1}, \ldots, \theta_{n}, v\right) \propto \prod_{i=1}^{n} N\left(x_{i} \mid 0,\left(e^{-v / 2}\right)^{2}\right) N\left(v \mid 0,3^{2}\right)
$$

-   This hierarchical structure induces large correlations between $v$ and each $\theta_{i}$

[Betancourt and Girolami 2013]{.footerRight}

## Visualizing the pathology ðŸ˜ˆ

![](images/funnel.png){fig-align="center" width="800" height="600"}

## Some things worth noting

-   There is position dependence in the correlation structure, i.e., correlation changes depending on where you are located in the posterior

[Betancourt and Girolami 2013]{.footerRight}

## Some things worth noting

-   There is position dependence in the correlation structure, i.e., correlation changes depending on where you are located in the posterior
-   No global correction, like rotating or rescaling will solve this problem!

[Betancourt and Girolami 2013]{.footerRight}

## Some things worth noting

-   There is position dependence in the correlation structure, i.e., correlation changes depending on where you are located in the posterior
-   No global correction, like rotating or rescaling will solve this problem!
-   Often manifests as a divergent transition in Stan, as HMC cannot accurately explore the posterior

[Betancourt and Girolami 2013]{.footerRight}

## How can we fix this problem?

-   Remember that the prior information we include in an analysis is not only limited to the choice of an explict prior distribution

[Betancourt and Girolami 2013]{.footerRight}

## How can we fix this problem?

-   Remember that the prior information we include in an analysis is not only limited to the choice of an explict prior distribution
-  The dependence between layers in our model can actually be broken up by reparameterizing the existing parameters into a so-called "non-centered" parameterization
    - Think about the DAG
[Betancourt and Girolami 2013]{.footerRight}

## How can we fix this problem?

-   Remember that the prior information we include in an analysis is not only limited to the choice of an explict prior distribution
-  The dependence between layers in our model can actually be broken up by reparameterizing the existing parameters into a so-called "non-centered" parameterization
    - Think about the DAG
-  Non-centered parameterizations factor certain dependencies into deterministic
transformations between the layers, leaving the actively sampled variables uncorrelated

[Betancourt and Girolami 2013]{.footerRight}

## Centered vs. non-centered model maths 

::: {style="font-size: 85%;"}

Centered model:
$$
\begin{aligned}
y_{i} & \sim {N}\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim {N}\left(\mu, \tau^{2}\right), \end{aligned}
$$

Non-centered analog:

:::

[Betancourt and Girolami 2013]{.footerRight}

## Centered vs. non-centered model maths 

::: {style="font-size: 85%;"}

Centered model:
$$
\begin{aligned}
y_{i} & \sim {N}\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim {N}\left(\mu, \tau^{2}\right), \end{aligned}
$$

Non-centered analog:

$$
\begin{aligned}
y_{i} & \sim N\left(\vartheta_{i} \tau+\mu, \sigma_{i}^{2}\right) \\
\vartheta_{i} & \sim N(0,1).
\end{aligned}
$$


:::

[Betancourt and Girolami 2013]{.footerRight}

## Centered vs. non-centered model maths 

::: {style="font-size: 85%;"}

Centered model:
$$
\begin{aligned}
y_{i} & \sim {N}\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim {N}\left(\mu, \tau^{2}\right), \end{aligned}
$$

Non-centered analog:

$$
\begin{aligned}
y_{i} & \sim N\left(\vartheta_{i} \tau+\mu, \sigma_{i}^{2}\right) \\
\vartheta_{i} & \sim N(0,1).
\end{aligned}
$$

Key point: NCP shifts correlation from the latent parameters to data 

:::

[Betancourt and Girolami 2013]{.footerRight}

## Centered vs. non-centered model DAG 

![](images/DAG2.png){fig-align="center" width="800" height="500"}

[Betancourt and Girolami 2013]{.footerRight}

## When does NCP help? 

![](images/timing.png){fig-align="center" width="450" height="500"}

[Betancourt and Girolami 2013]{.footerRight}

## Example

![](images/ghosts.jpg){fig-align="center" width="900" height="500"}

## Example 

- consider the one-way normal model with 800 latent $\theta_{i}$
- constant measurement error $\sigma_{i} = \sigma = 10$
- latent parameters are $\mu = 8, \tau = 3$
- $\theta_{i}$ and $y_{i}$ sampled randomly 

<br>

[Betancourt and Girolami 2013]{.footerRight}

## Example 

- consider the one-way normal model with 800 latent $\theta_{i}$
- constant measurement error $\sigma_{i} = \sigma = 10$
- latent parameters are $\mu = 8, \tau = 3$
- $\theta_{i}$ and $y_{i}$ sampled randomly 

<br>

Add weakly informative priors to this generative likelihood

$$
\begin{array}{l}
\pi(\mu)=N\left(0,5^{2}\right) \\
\pi(\tau)=\text { Half-Cauchy }(0,2.5) .
\end{array}
$$

[Betancourt and Girolami 2013]{.footerRight}

## Example, centered vs. noncentered 

::: {style="font-size: 85%;"}

The centered parameterization of this model can be written as

$$
\begin{aligned}
y_{i} & \sim N\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim N\left(\mu, \tau^{2}\right), \text { for } i=1, \ldots, 800
\end{aligned}
$$

:::

[Betancourt and Girolami 2013]{.footerRight}

## Example, centered vs. noncentered 

::: {style="font-size: 85%;"}

The centered parameterization of this model can be written as

$$
\begin{aligned}
y_{i} & \sim N\left(\theta_{i}, \sigma_{i}^{2}\right) \\
\theta_{i} & \sim N\left(\mu, \tau^{2}\right), \text { for } i=1, \ldots, 800
\end{aligned}
$$

and it should have inferior performance relative to the noncentered model: 

$$
\begin{aligned}
y_{i} & \sim N\left(\tau \vartheta_{i}+\mu, \sigma_{i}^{2}\right) \\
\vartheta_{i} & \sim N(0,1), \text { for } i=1, \ldots, 800
\end{aligned}
$$

:::

[Betancourt and Girolami 2013]{.footerRight}

## Using Stan to simulate fake data  

::: {style="font-size: 75%;"}

```{R echo = T, eval = F}
transformed data {
  real mu;
  real<lower=0> tau;
  real alpha;
  int N;
  mu = 8;
  tau = 3;
  alpha = 10;
  N = 800;
}
generated quantities {
  real mu_print;
  real tau_print;
  vector[N] theta;
  vector[N] sigma;
  vector[N] y;
  mu_print = mu;
  tau_print = tau;
  for (i in 1:N) {
    theta[i] = normal_rng(mu, tau);
    sigma[i] = alpha;
    y[i] = normal_rng(theta[i], sigma[i]);
  }
}
```
:::

## Calling that from R 

```{R echo = T, eval = T}

library("cmdstanr")   

one_level <- cmdstan_model("src/sim_one_level.stan")

# simulate data
sim <- one_level$sample(
  fixed_param = T, # look here 
  iter_warmup = 0, iter_sampling = 1,
  chains = 1, seed = 1
)

```

- Note the fixed_param and iters

## Extract the relevant quantities 

```{R echo = T, eval = T}
# extract it 
y <- as.vector(sim$draws("y", format = "draws_matrix"))
sigma <- as.vector(sim$draws("sigma", format = "draws_matrix"))
theta <- as.vector(sim$draws("theta", format = "draws_matrix"))
mu <- as.vector(sim$draws("mu_print", format = "draws_matrix"))
tau <- as.vector(sim$draws("tau_print", format = "draws_matrix"))
```

## Look at it (duh) 

```{R echo = T, eval = T}
library(tidyverse)
my_data <- data.frame(y, group = 1:length(y))
my_data %>%
    ggplot(aes(x = group, y = y)) + 
    geom_point()
```

## The centered parameterization in code

::: {style="font-size: 85%;"}

```{R echo = T, eval = F}
data {
  int<lower=0> J;
  array[J] real y;
  array[J] real sigma;
}
parameters {
  real mu;
  real<lower=0> tau;
  array[J] real theta;
}
model {
  mu ~ normal(0, 5);
  tau ~ cauchy(0, 2.5);
  theta ~ normal(mu, tau);
  y ~ normal(theta, sigma);
}
```
:::

## The noncentered parameterization in code

::: {style="font-size: 85%;"}

```{R echo = T, eval = F}
data {
  int<lower=0> J;
  array[J] real y;
  array[J] real sigma;
}
parameters {
  real mu;
  real<lower=0> tau;
  array[J] real var_theta;
}
transformed parameters {
  array[J] real theta;
  for (j in 1:J) theta[j] = tau * var_theta[j] + mu;
}
model {
  mu ~ normal(0, 5);
  tau ~ cauchy(0, 2.5);
  var_theta ~ normal(0, 1);
  y ~ normal(theta, sigma);
}
```
:::

## Running things from R

::: {style="font-size: 85%;"}


```{R echo = T, eval = T}
# Centered estimation model: 
stan_data <- list(
  J = length(y), y = y, sigma = sigma
)
one_level_cp <- cmdstan_model("src/one_level_cp.stan")

fit_cp <- one_level_cp$sample(
  data = stan_data,
  iter_warmup = 1000, iter_sampling = 1000,
  chains = 4, parallel_chains = 4,
  seed = 13, refresh = 0, adapt_delta = 0.99
)

```

:::

## Checking diagnostics

::: {style="font-size: 85%;"}

```{R echo = T, eval = T}
fit_cp$cmdstan_diagnose()
```

:::

## Running things from R

::: {style="font-size: 85%;"}

```{R echo = T, eval = T}
# noncentered estimation model: 
one_level_ncp <- cmdstan_model("src/one_level_ncp.stan")

fit_ncp <- one_level_ncp$sample(
  data = stan_data,
  iter_warmup = 1000, iter_sampling = 1000,
  chains = 4, parallel_chains = 4,
  seed = 13, refresh = 0, adapt_delta = 0.99
)

```
:::

## Checking diagnostics

::: {style="font-size: 85%;"}

```{R echo = T, eval = T}
fit_ncp$cmdstan_diagnose()
```

:::

## Comparing the two models

```{R echo = T, eval = T}
fit_cp$summary(c("mu", "tau"))
fit_ncp$summary(c("mu", "tau"))

```

## First example wrap up 

- The centered parameterization throws low-EBFMI warnings, occasional divergent transition warnings, and maximum treedepth reached warnings
- NCP increases efficiency (measured as ESS / run time)

## Eight schools in class demo 

::: {style="font-size: 85%;"}

Centered model:
$$
\begin{aligned}
y_{j} & \sim \operatorname{Normal}\left(\theta_{j}, \sigma_{j}\right), \quad j=1, \ldots, J \\
\theta_{j} & \sim \operatorname{Normal}(\mu, \tau), \quad j=1, \ldots, J \\
\mu & \sim \operatorname{Normal}(0,10) \\
\tau & \sim \operatorname{half}-\operatorname{Cauchy}(0,10)
\end{aligned}
$$

Non-centered analog:

$$
\begin{aligned}\theta _j&=\ \mu +\tau \eta _j,\quad j=1,\ldots ,J\\
\eta _j&\sim N(0,1),\quad j=1,\ldots ,J.\end{aligned}
$$

:::

[Rubin 1981, Gelman et al. 2013; Stan Development Team 2023]{.footerRight}

## Key thing to note about Eight Schools NCP vs. CP

- NCP replaces the vector $\theta$ with a vector $\eta$ of i.i.d. standard normal parameters and then constructs $\theta$ deterministically from $\eta$ by scaling by $\tau$ and shifting by $\mu$ 

[Rubin 1981, Gelman et al. 2013; Stan Development Team 2023]{.footerRight}

## Key thing to note about Eight Schools NCP vs. CP

- NCP replaces the vector $\theta$ with a vector $\eta$ of i.i.d. standard normal parameters and then constructs $\theta$ deterministically from $\eta$ by scaling by $\tau$ and shifting by $\mu$ 

- To the code!

[Rubin 1981, Gelman et al. 2013; Stan Development Team 2023]{.footerRight}

## References

- Betancourt, M. and Girolami, M. 2013. Hamiltonian Monte Carlo for hierarchical models. https://arxiv.org/abs/1312.0906

- Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. 2013. Bayesian Data Analysis. Chapman & Hall/CRC Press, London, third edition.

- Rubin, D. B. 1981. Estimation in Parallel Randomized Experiments. Journal of Educational and Behavioral Statistics. 6:377â€“401.

- Stan Development Team. 2023. Stan Modeling Language Users Guide and Reference Manual. https://mc-stan.org/users/documentation/

