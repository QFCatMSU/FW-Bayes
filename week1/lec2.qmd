---
title: Approximating posterior distributions using simulation
title-slide-attributes:
    data-background-image: "https://qfcatmsu.github.io/Images/dark-big.png"  
    data-background-size: "40%"
    data-background-position: "6% 95%"
subtitle: FW 891
author: Christopher Cahill
date: 30 August 2023
date-format: "D MMMM YYYY"
format: 
  revealjs:
    css: "https://qfcatmsu.github.io/css/presStyle.css"
    slide-number: c/t  
    theme: simple 
highlight-style: kate
---

## Recap

-   Last time we learned that in some special situations we could generate analytical posterior distributions (e.g., the beta-binomial model)
-   However, the reality is that this only works for simple models (i.e., typically \< 3 parameters)
-   Today we explore a broad class of computational methods and algorithms aimed at approximating posterior distributions
-   We will still use simple models today, but realize that some of these tools scale far better than others to high-dimensional space

## Outline

-   Computing a posterior via grid approximation\
-   Markov Chain Monte Carlo
-   Metropolis-Hastings algorithm
-   Hamiltonian Monte Carlo

## Grid search approximation: the basics

-   Recall that: 
$$
    \color{darkgreen}{posterior} \propto \color{#E78021}{likelihood} \cdot \color{#3697DC}{prior}
$$

-   For models with 1-2 parameters approximating the posterior is actually a fairly tractable problem, even when the math isn't nice

-   All we do is create a "grid" of parameter values and compute the posterior

-   We then search across the computed posterior values to find the posterior maximum and corresponding parameter estimates

## Revisiting the sneak turtles with different priors

```{R echo = T}
library(tidyverse)
library(ggqfc)
y <- c(
  0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
) # sucesses (turtle detections)
n <- length(y) # trials

# define a grid
grid_size <- 10
p_grid <- seq(from = 1e-3, to = 0.999, length.out = grid_size)

# calculate likelihood
lik <- dbinom(sum(y), n, p_grid)
lik # likelihood of having observed the data | each p_grid value
```

-   see ?dunif, ?dnorm, ?dbinom, etc.

## [Always]{style="text-decoration: underline;"} work in log-space when dealing with likelihoods and prior probabilities!

Recognize that 
$$
\color{darkgreen}{posterior} \propto \color{#E78021}{likelihood} \cdot \color{#3697DC}{prior}
$$

is the same as 
$$
\color{darkgreen}{log(posterior)} \propto \color{#E78021}{log(likelihood)} + \color{#3697DC}{log(prior)}
$$

When we multiply likelihoods together they get smaller and may surpass machine precision, so we usually work in log-space

## Back to the R side of things...

```{R echo = T}
# Note that
log(lik)

# is the same as
log_lik <- dbinom(sum(y), n, p_grid, TRUE)
log_lik
```

-   We have now defined the log-likelihood across our entire grid sequence of $p$
-   Now we need to calculate the prior probabilities at each `p_grid` value

## Dealing with the prior probablities

-   Assume $p\sim \mathrm{Uniform}(0, 1)$
-   We can calculate the log(prior probability) for each $p$ in our grid given a uniform prior via:

```{R echo = T}
log_prior <- dunif(p_grid, 0, 1, TRUE)
log_prior
```

-   Is this correct?

## Visualizing the Uniform prior

```{R echo = F}
my_df <- data.frame(log_prior, p_grid)
p <- my_df %>%
  ggplot(aes(x = p_grid, y = exp(log_prior))) +
  geom_line() +
  xlab("value of p") +
  ylab("probability") +
  theme_qfc() +
  theme(text = element_text(size = 20))
p
```

## Calculating the posterior

-   Now can calculate the posterior probabilities of each `p_grid` value given the likelihood and prior as follows:

```{R echo = T}
# unstandardized log posterior
log_unstd_post <- log_lik + log_prior

# exponentiate the log posterior
unstd_post <- exp(log_unstd_post)

# standardize it (so probabilities sum to one)
post_prob <- unstd_post / sum(unstd_post)

# Put everything together in a data frame
posterior <- data.frame(
  p = p_grid, log_lik, log_prior,
  log_unstd_post, post_prob
)
```

## Let's take a look at our posterior approximation

```{R echo = T}
posterior
which.max(posterior$post_prob)
posterior[which.max(posterior$post_prob), ] # best estimate of p
```

## Key points:

For each value of $p$ in `p_grid` we get

-   log-likelihood of the data \| that $p$
-   log-prior probability for that $p$ \| the prior(s)
-   log-posterior calculation for each $p$ (very much not a probability)
-   posterior probability for each $p$

## Key points continued:

-   The log-likelihood refers to the joint density of all data points (i.e., the sum of the log-likelihood values for each datum)

-   log-prior is the marginal distribution of the parameter(s)

## Comparing our grid approximation with the analytical MLE

```{R echo=F}
MAP <- posterior[which.max(posterior$post_prob), ]$p # best estimate of p
posterior %>%
  ggplot(aes(x = p, y = post_prob)) +
  geom_point() +
  geom_line() +
  ggtitle(paste0("Grid size: ", grid_size, " points")) +
  xlab(expression(detection ~ probability ~ p)) +
  ylab("posterior probability") +
  geom_vline(aes(
    xintercept = MAP,
    color = "grid estimate"
  ), lty = 2, lwd = 2) +
  geom_vline(aes(xintercept = sum(y) / n, color = "analytical MLE"),
    lty = 1, lwd = 2, alpha = 0.6
  ) +
  scale_color_manual(
    name = "",
    values = c(
      `grid estimate` = "steelblue",
      `analytical MLE` = "darkorange2"
    )
  ) +
  theme_qfc() +
  theme(legend.position = c(0.85, 0.9), text = element_text(size = 20))
```

## Comparing our grid approximation with the analytical MLE

```{R echo=F}
# write a function to do this
get_post <- function(grid_size = 10) {
  p_grid <- seq(from = 1e-3, to = 0.999, length.out = grid_size)

  # calculate likelihood
  lik <- dbinom(sum(y), n, p_grid)
  log_lik <- dbinom(sum(y), n, p_grid, TRUE)
  log_prior <- dunif(p_grid, 0, 1, TRUE)

  # unstandardized log posterior
  log_unstd_post <- log_lik + log_prior

  # exponentiate the log posterior
  unstd_post <- exp(log_unstd_post)

  # standardize it (so probabilities sum to one)
  post_prob <- unstd_post / sum(unstd_post)

  # Put everything together in a data frame
  posterior <- data.frame(
    p = p_grid, log_lik, log_prior,
    log_unstd_post, post_prob
  )
  posterior
}

grid_size <- 20
posterior <- get_post(grid_size = grid_size)
MAP <- posterior[which.max(posterior$post_prob), ]$p # best estimate of p

posterior %>%
  ggplot(aes(x = p, y = post_prob)) +
  geom_point() +
  geom_line() +
  ggtitle(paste0("Grid size: ", grid_size, " points")) +
  xlab(expression(detection ~ probability ~ p)) +
  ylab("posterior probability") +
  geom_vline(aes(
    xintercept = MAP,
    color = "grid estimate"
  ), lty = 2, lwd = 2) +
  geom_vline(aes(xintercept = sum(y) / n, color = "analytical MLE"),
    lty = 1, lwd = 2, alpha = 0.6
  ) +
  scale_color_manual(
    name = "",
    values = c(
      `grid estimate` = "steelblue",
      `analytical MLE` = "darkorange2"
    )
  ) +
  theme_qfc() +
  theme(legend.position = c(0.85, 0.9), text = element_text(size = 20))
```

## Comparing our grid approximation with the analytical MLE

```{R echo = F}
grid_size <- 30
posterior <- get_post(grid_size = grid_size)
MAP <- posterior[which.max(posterior$post_prob), ]$p # best estimate of p

posterior %>%
  ggplot(aes(x = p, y = post_prob)) +
  geom_point() +
  geom_line() +
  ggtitle(paste0("Grid size: ", grid_size, " points")) +
  xlab(expression(detection ~ probability ~ p)) +
  ylab("posterior probability") +
  geom_vline(aes(
    xintercept = MAP,
    color = "grid estimate"
  ), lty = 2, lwd = 2) +
  geom_vline(aes(xintercept = sum(y) / n, color = "analytical MLE"),
    lty = 1, lwd = 2, alpha = 0.6
  ) +
  scale_color_manual(
    name = "",
    values = c(
      `grid estimate` = "steelblue",
      `analytical MLE` = "darkorange2"
    )
  ) +
  theme_qfc() +
  theme(legend.position = c(0.85, 0.9), text = element_text(size = 20))
```

## Comparing our grid approximation with the analytical MLE

```{R echo = F}
grid_size <- 100
posterior <- get_post(grid_size = grid_size)
MAP <- posterior[which.max(posterior$post_prob), ]$p # best estimate of p

posterior %>%
  ggplot(aes(x = p, y = post_prob)) +
  geom_point() +
  geom_line() +
  ggtitle(paste0("Grid size: ", grid_size, " points")) +
  xlab(expression(detection ~ probability ~ p)) +
  ylab("posterior probability") +
  geom_vline(aes(
    xintercept = MAP,
    color = "grid estimate"
  ), lty = 2, lwd = 2) +
  geom_vline(aes(xintercept = sum(y) / n, color = "analytical MLE"),
    lty = 1, lwd = 2, alpha = 0.6
  ) +
  scale_color_manual(
    name = "",
    values = c(
      `grid estimate` = "steelblue",
      `analytical MLE` = "darkorange2"
    )
  ) +
  theme_qfc() +
  theme(legend.position = c(0.85, 0.9), text = element_text(size = 20))
```

## Comparing our grid approximation with the analytical MLE

```{R echo = F}
grid_size <- 1000
posterior <- get_post(grid_size = grid_size)
MAP <- posterior[which.max(posterior$post_prob), ]$p # best estimate of p

posterior %>%
  ggplot(aes(x = p, y = post_prob)) +
  geom_point() +
  geom_line() +
  ggtitle(paste0("Grid size: ", grid_size, " points")) +
  xlab(expression(detection ~ probability ~ p)) +
  ylab("posterior probability") +
  geom_vline(aes(
    xintercept = MAP,
    color = "grid estimate"
  ), lty = 2, lwd = 2) +
  geom_vline(aes(xintercept = sum(y) / n, color = "analytical MLE"),
    lty = 1, lwd = 2, alpha = 0.6
  ) +
  scale_color_manual(
    name = "",
    values = c(
      `grid estimate` = "steelblue",
      `analytical MLE` = "darkorange2"
    )
  ) +
  theme_qfc() +
  theme(legend.position = c(0.85, 0.9), text = element_text(size = 20))
```

## Exercise

-   A new study estimated the detection probability of sneak turtles, and you want to incorporate this information as prior information into your analysis
-   Repeat the analysis above in R with a grid_size of 10000 and a $p\sim \mathrm{Normal}(0.13, 0.12)$
-   Report your best estimate of log-posterior and p given this prior
-   If you get this far, try a bunch of different normal priors and think about how it influences your findings

## Solution

```{R echo = F}
p_grid <- seq(from = 1e-3, to = 0.999, length.out = 10000)
log_lik <- dbinom(sum(y), n, p_grid, TRUE)
log_prior <- dnorm(p_grid, 0.13, 0.12, TRUE)
log_unstd_post <- log_lik + log_prior
unstd_post <- exp(log_unstd_post)
post_prob <- unstd_post / sum(unstd_post)

# Put everything together in a data frame
posterior <- data.frame(
  p = p_grid, log_lik, log_prior,
  log_unstd_post, post_prob
)

posterior[which.max(posterior$post_prob), ]
```

## Why can't we just grid search everything?

-   There are many important model types (e.g., mixed effects models) for which analytical solutions and things like grid appoximation fail miserably
    -   For models with many parameters and a dense grid, computational burden is intractable
    -   This includes almost all models relevant to applied ecology

## The Bayesian problem

$$
P(\theta \mid y)=\frac{P(y \mid \theta) P(\theta)}{P(y)}
$$

-   $P(y)$ is called the "evidence" (i.e. the evidence that the data y was generated by this model)

-   We can compute this quantity by integrating over all possible parameter values:

$$
P(y)=\int_{\Theta} P(y, \theta) \mathrm{d} \theta
$$

## The Bayesian problem cont'd

-   This is *the* key difficulty with Bayesian statistics, and is why Bayesian statistics was not popular until computing power increased in the 1990s
    -   Even for relatively simple models this thing can be intractable
-   If we can't solve it, can we approximate it via [Monte Carlo](https://en.wikipedia.org/wiki/Monte_Carlo_method)?
    -   Monte Carlo = repeated random sampling
-   Unfortunately this would require us to first solve Bayes' theorem because we don't know what the distribution $P(\theta \mid y)$ is

[McElreath 2023 chapter 2]{.footerRight}

## The Bayesian problem cont'd

-   So we can't compute it using standard methods, can't sample directly from it\
-   Mathematicians realized this, and instead said:
    -   "Let's construct an [ergodic](https://en.wikipedia.org/wiki/Ergodic_theory), reversible [Markov chain](https://en.wikipedia.org/wiki/Markov_chain) that has as an equilibrium distribution which matches our posterior distribution"
-   What? ðŸ¦‡ðŸ’©
-   Markov chain: what happens next depends only on the state of affairs now
-   The remarkable fact is that this is relatively easy to do [Gelman et al. 2003]{.footerRight}

## Introduction to Markov Chain Monte Carlo (MCMC)

-   What is MCMC: a broad class of algorithms that allow us to sample from an arbitrary probability distribution

-   Counter-intuitive to most non-robots

-   Rather than computing the $P(\theta \mid y)$ directly, MCMC draws samples of parameters from $P(\theta \mid y)$

    -   This is actually **extremely** useful

-   Collecting enough samples allows us to 'map out' or get a picture of $P(\theta \mid y)$

[see also McElreath 2023 chapter 2]{.footerRight}

## The Metropolis-Hastings algorithm

-   Metropolis--Hastings (MH) is a classic MCMC algorithm with the following rules:

-   Let $f(x)$ be a function that is *proportional* to the desired probability density function $P(x)$ (a.k.a the "target distribution")

-   Remember that we know:

$$
    \color{darkgreen}{posterior} \propto \color{#E78021}{likelihood} \cdot \color{#3697DC}{prior}
$$

-   So we just set $f(x)$ = $P(y \mid \theta) \cdot P(\theta)$

## The Metropolis-Hastings algorithm cont'd

The recipe:

-   Choose an arbitrary starting value for a parameter $x_{t}$
-   Choose an arbitrary probability distribution $g(x \mid y)$ to *propose* the next parameter value $x$ given the previous value $y$
    -   Note $g(x \mid y)$ is usually a Gaussian distribution centered at $y$ so that points closer to $y$ are more likely to be visited next (random walk)

[Gelman et al. 2003]{.footerRight}

## The Metropolis-Hastings algorithm cont'd

-   for each iteration $t$

    -   generate a proposed value for $x^{\prime}$ by drawing a random number from $g\left(x^{\prime} \mid x_{t}\right)$

    -   calculate the acceptance probability (a.k.a. acceptance ratio) $\alpha=f\left(x^{\prime}\right) / f\left(x_{t}\right)$

    -   accept or reject the proposed value $x^{\prime}$

        -   generate a uniform random number $u \in[0,1]$
        -   if u \> $\alpha$, accept the proposal and set $x_{+1}=x^{\prime}$
        -   if u \< $\alpha$, reject the proposal and set $x_{+1}=x_{t}$

[Gelman et al. 2003]{.footerRight}

## Re-thinking the acceptance probability

This thing works because

$\alpha=f\left(x^{\prime}\right) / f\left(x_{t}\right)$

is the same as saying

$$
\alpha=\frac{\frac{P(y \mid \theta^{\prime}) P(\theta^{\prime})}{P(y)}}{\frac{P(y \mid \theta_{t}) P(\theta_{t})}{P(y)}}
$$

Note we just substituted $x^{\prime}$ and $x_{t}$ for $\theta^{\prime}$ and $\theta_{t}$

-   $P(y)$ cancels out via mathemagicks

## Remember the sneak turtles ðŸ¢!

```{R echo = T}
y
n
```

## Building a Metropolis-Hastings algorithm for sneak turtles ðŸ¢

First, write three helper functions:

```{R echo = T}
l_prior <- function(param) { # log(prior)
  dunif(param, min = 0, max = 1, log = T) # uniform(0,1) prior
}

l_lik <- function(param) { # log(likelihood)
  dbinom(sum(y), n, param, log = T)
}

l_post <- function(param) { # log(posterior) = log(likelihood) + log(prior)
  l_lik(param) + l_prior(param)
}
```

## Building a Metropolis-Hastings algorithm for sneak turtles ðŸ¢

Second, declare some stuff

```{R echo = T}
n_iter <- 10000 # number of iterations to run the MCMC
chain <- rep(NA, n_iter) # place to store values of p
chain[1] <- 0.5 # initial guess at p
```

## Building a Metropolis-Hastings algorithm for sneak turtles ðŸ¢

Now we run the algorithm

```{R, echo = T}
for (i in 2:n_iter) {
  p_new <- rnorm(1, chain[i - 1], 0.1) # new proposed p based on previous p

  # next two lines ensure new p stays between [0,1] - ignore them
  if (p_new < 0) p_new <- abs(p_new)
  if (p_new > 1) p_new <- 2 - p_new

  a_prob <- exp(l_post(p_new) - l_post(chain[i - 1])) # acceptance prob

  if (runif(1) < a_prob) {
    chain[i] <- p_new # accept the proposal
  } else {
    chain[i] <- chain[i - 1] # reject the proposal
  }
}
```

[see also Gelman et al. 2003; Robert and Casella 2010]{.footerRight}

## Metropolis-Hastings results

```{R echo = F}
my_df <- data.frame(p = chain, iter = 1:n_iter)

my_df %>%
  ggplot(aes(x = p)) +
  geom_histogram() +
  geom_vline(aes(xintercept = sum(y) / n, color = "analytical MLE"),
    lty = 1, lwd = 2, alpha = 0.6
  ) +
  scale_fill_manual(values = c(`MCMC draws` = "gray80")) +
  scale_color_manual(
    name = "",
    values = c(
      `analytical MLE` = "darkorange2"
    )
  ) +
  ggtitle("Gray histogram shows draws of p") +
  theme_qfc() +
  theme(
    legend.position = c(0.85, 0.9), text = element_text(size = 20),
    legend.background = element_rect(fill = "white")
  )
```

## Metropolis-Hastings results

```{R echo = F}
my_df %>%
  ggplot(aes(x = iter, y = p)) +
  geom_line() +
  theme_qfc() +
  xlab("iteration of algorithm") +
  ylab("value of p") +
  theme(
    legend.position = c(0.85, 0.9), text = element_text(size = 20),
    legend.background = element_rect(fill = "white")
  )
```

## Exercise

Get in groups of three, play with the following:

-   the proposal distribution
-   the initialization value
-   chain length
-   plot histogram of p and the chain (the fuzzy catepillar plot)

Report your findings back to the group

## MCMC algorithm diagnostics

-   Many of the parameters we were playing with affect our approximation of the posterior!
-   Remember, this is a dead-simple example. Eventually we will use MCMC to approximate high-dimensional integrals which is much more difficult
-   Clearly takes some time to "converge" on a stable posterior distribution
    -   With MH MCMC this period between initial values and convergence to some distribution is known as the "burn-in" period, and we usually discard it

[Gelman et al. 2003; Kery and Schaub 2013; McElreath et al. 2023]{.footerRight}

## Proposal distribution sd = 0.001

```{R, echo = F}
get_chain <- function(proposal_sd = 0.001, init = 0.5) { # wrap it in a function for ease
  n_iter <- 10000 # number of iterations to run the MCMC
  chain <- rep(NA, n_iter) # place to store values of p
  chain[1] <- init # initial guess at p

  for (i in 2:n_iter) {
    p_new <- rnorm(1, chain[i - 1], proposal_sd) # new proposed p based on previous p

    # next two lines ensure new p stays between [0,1] - ignore them
    if (p_new < 0) p_new <- abs(p_new)
    if (p_new > 1) p_new <- 2 - p_new

    a_prob <- exp(l_post(p_new) - l_post(chain[i - 1])) # acceptance prob

    if (runif(1) < a_prob) {
      chain[i] <- p_new # accept the proposal
    } else {
      chain[i] <- chain[i - 1] # reject the proposal
    }
  }
  chain
}

chain <- get_chain(proposal_sd = 0.001, init = )
my_df <- data.frame(p = chain, iter = 1:n_iter)

my_df %>%
  ggplot(aes(x = iter, y = p)) +
  geom_line() +
  theme_qfc() +
  ylim(0, 1) +
  annotate("rect", xmin = -Inf, xmax = 5000, ymin = -Inf, ymax = Inf, alpha = 0.2, fill = "gray") +
  geom_hline(aes(yintercept = sum(y) / n, color = "analytical MLE"),
    lty = 1, lwd = 2, alpha = 0.6
  ) +
  scale_color_manual(
    name = "",
    values = c(
      `analytical MLE` = "darkorange2"
    )
  ) +
  xlab("iteration of algorithm") +
  ylab("value of p") +
  theme(
    legend.position = c(0.85, 0.9), text = element_text(size = 20),
    legend.background = element_rect(fill = "white")
  )
```

## Proposal distribution sd = 0.002

```{R, echo = F}
chain <- get_chain(proposal_sd = 0.002, init = 0.5)
my_df <- data.frame(p = chain, iter = 1:n_iter)

my_df %>%
  ggplot(aes(x = iter, y = p)) +
  geom_line() +
  theme_qfc() +
  ylim(0, 1) +
  annotate("rect", xmin = -Inf, xmax = 5000, ymin = -Inf, ymax = Inf, alpha = 0.2, fill = "gray") +
  geom_hline(aes(yintercept = sum(y) / n, color = "analytical MLE"),
    lty = 1, lwd = 2, alpha = 0.6
  ) +
  scale_color_manual(
    name = "",
    values = c(
      `analytical MLE` = "darkorange2"
    )
  ) +
  xlab("iteration of algorithm") +
  ylab("value of p") +
  theme(
    legend.position = c(0.85, 0.9), text = element_text(size = 20),
    legend.background = element_rect(fill = "white")
  )
```

## Proposal distribution sd = 0.005

```{R, echo = F}
chain <- get_chain(proposal_sd = 0.005, init = 0.5)
my_df <- data.frame(p = chain, iter = 1:n_iter)

my_df %>%
  ggplot(aes(x = iter, y = p)) +
  geom_line() +
  theme_qfc() +
  ylim(0, 1) +
  annotate("rect", xmin = -Inf, xmax = 5000, ymin = -Inf, ymax = Inf, alpha = 0.2, fill = "gray") +
  geom_hline(aes(yintercept = sum(y) / n, color = "analytical MLE"),
    lty = 1, lwd = 2, alpha = 0.6
  ) +
  scale_color_manual(
    name = "",
    values = c(
      `analytical MLE` = "darkorange2"
    )
  ) +
  xlab("iteration of algorithm") +
  ylab("value of p") +
  theme(
    legend.position = c(0.85, 0.9), text = element_text(size = 20),
    legend.background = element_rect(fill = "white")
  )
```

## Proposal distribution sd = 0.01

```{R, echo = F}
chain <- get_chain(proposal_sd = 0.01, init = 0.5)
my_df <- data.frame(p = chain, iter = 1:n_iter)

my_df %>%
  ggplot(aes(x = iter, y = p)) +
  geom_line() +
  theme_qfc() +
  ylim(0, 1) +
  annotate("rect", xmin = -Inf, xmax = 5000, ymin = -Inf, ymax = Inf, alpha = 0.2, fill = "gray") +
  geom_hline(aes(yintercept = sum(y) / n, color = "analytical MLE"),
    lty = 1, lwd = 2, alpha = 0.6
  ) +
  scale_color_manual(
    name = "",
    values = c(
      `analytical MLE` = "darkorange2"
    )
  ) +
  xlab("iteration of algorithm") +
  ylab("value of p") +
  theme(
    legend.position = c(0.85, 0.9), text = element_text(size = 20),
    legend.background = element_rect(fill = "white")
  )
```

## Proposal distribution sd = 0.1

```{R, echo = F}
chain <- get_chain(proposal_sd = 0.1, init = 0.5)
my_df <- data.frame(p = chain, iter = 1:n_iter)

my_df %>%
  ggplot(aes(x = iter, y = p)) +
  geom_line() +
  theme_qfc() +
  ylim(0, 1) +
  annotate("rect", xmin = -Inf, xmax = 5000, ymin = -Inf, ymax = Inf, alpha = 0.2, fill = "gray") +
  geom_hline(aes(yintercept = sum(y) / n, color = "analytical MLE"),
    lty = 1, lwd = 2, alpha = 0.6
  ) +
  scale_color_manual(
    name = "",
    values = c(
      `analytical MLE` = "darkorange2"
    )
  ) +
  xlab("iteration of algorithm") +
  ylab("value of p") +
  theme(
    legend.position = c(0.85, 0.9), text = element_text(size = 20),
    legend.background = element_rect(fill = "white")
  )
```

## Proposal distribution sd = 0.2

```{R, echo = F}
chain <- get_chain(proposal_sd = 0.2, init = 0.5)
my_df <- data.frame(p = chain, iter = 1:n_iter)

my_df %>%
  ggplot(aes(x = iter, y = p)) +
  geom_line() +
  theme_qfc() +
  ylim(0, 1) +
  annotate("rect", xmin = -Inf, xmax = 5000, ymin = -Inf, ymax = Inf, alpha = 0.2, fill = "gray") +
  geom_hline(aes(yintercept = sum(y) / n, color = "analytical MLE"),
    lty = 1, lwd = 2, alpha = 0.6
  ) +
  scale_color_manual(
    name = "",
    values = c(
      `analytical MLE` = "darkorange2"
    )
  ) +
  xlab("iteration of algorithm") +
  ylab("value of p") +
  theme(
    legend.position = c(0.85, 0.9), text = element_text(size = 20),
    legend.background = element_rect(fill = "white")
  )
```

## Proposal distribution sd = 0.4

```{R, echo = F}
chain <- get_chain(proposal_sd = 0.4, init = 0.5)
my_df <- data.frame(p = chain, iter = 1:n_iter)

my_df %>%
  ggplot(aes(x = iter, y = p)) +
  geom_line() +
  theme_qfc() +
  ylim(0, 1) +
  annotate("rect", xmin = -Inf, xmax = 5000, ymin = -Inf, ymax = Inf, alpha = 0.2, fill = "gray") +
  geom_hline(aes(yintercept = sum(y) / n, color = "analytical MLE"),
    lty = 1, lwd = 2, alpha = 0.6
  ) +
  scale_color_manual(
    name = "",
    values = c(
      `analytical MLE` = "darkorange2"
    )
  ) +
  xlab("iteration of algorithm") +
  ylab("value of p") +
  theme(
    legend.position = c(0.85, 0.9), text = element_text(size = 20),
    legend.background = element_rect(fill = "white")
  )
```

## Proposal distribution sd = 0.42

```{R, echo = F}
set.seed(1)
chain <- get_chain(proposal_sd = 0.42, init = 0.5)
my_df <- data.frame(p = chain, iter = 1:n_iter)

my_df %>%
  ggplot(aes(x = iter, y = p)) +
  geom_line() +
  theme_qfc() +
  ylim(0, 1) +
  annotate("rect", xmin = -Inf, xmax = 5000, ymin = -Inf, ymax = Inf, alpha = 0.2, fill = "gray") +
  geom_hline(aes(yintercept = sum(y) / n, color = "analytical MLE"),
    lty = 1, lwd = 2, alpha = 0.6
  ) +
  scale_color_manual(
    name = "",
    values = c(
      `analytical MLE` = "darkorange2"
    )
  ) +
  xlab("iteration of algorithm") +
  ylab("value of p") +
  theme(
    legend.position = c(0.85, 0.9), text = element_text(size = 20),
    legend.background = element_rect(fill = "white")
  )
```

## Proposal distribution sd = 0.44

```{R, echo = F}
chain <- get_chain(proposal_sd = 0.44, init = 0.5)
my_df <- data.frame(p = chain, iter = 1:n_iter)

my_df %>%
  ggplot(aes(x = iter, y = p)) +
  geom_line() +
  theme_qfc() +
  ylim(0, 1) +
  annotate("rect", xmin = -Inf, xmax = 5000, ymin = -Inf, ymax = Inf, alpha = 0.2, fill = "gray") +
  geom_hline(aes(yintercept = sum(y) / n, color = "analytical MLE"),
    lty = 1, lwd = 2, alpha = 0.6
  ) +
  scale_color_manual(
    name = "",
    values = c(
      `analytical MLE` = "darkorange2"
    )
  ) +
  xlab("iteration of algorithm") +
  ylab("value of p") +
  theme(
    legend.position = c(0.85, 0.9), text = element_text(size = 20),
    legend.background = element_rect(fill = "white")
  )
```

## Proposal distribution sd \> 0.44

Breaks the following lines and dies because gives p outside \[0,1\]

```{R echo = T, eval = F}
if (p_new < 0) p_new <- abs(p_new)
if (p_new > 1) p_new <- 2 - p_new
```

-   There are smarter ways to keep a value between \[0,1\]
-   Key points:
    -   tuning the algorithm impacts its ability to sample the posterior
    -   correlation in the Markov chain means you don't get independent samples of the parameters of interest

## What do we want the chains to do

-   We want chains to look like fuzzy caterpillars

![](images/fuzzy.png){fig-align="center" width="500" height="300"}

-   Good indication that the MCMC is **efficiently** sampling from a maximum in the underlying distribution

## Fuzzy caterpillars are good indication that a chain is behaving, but...

-   We could be stuck in a local maximum
    -   If we ran it longer we might find a better solution
    -   One way to deal: run multiple, independent chains initialized from different starting locations
-   Should get overlapping fuzzy caterpillars if everything goes well
-   Because chains are independent, we often run them in parallel

[Gelman et al. 2003; Kery and Schaub 2013; McElreath et al. 2023]{.footerLeft}

## Running multiple chains each with different initialization values

```{R, echo = F}
set.seed(2)
chain1 <- get_chain(proposal_sd = 0.01, init = 0.3)
chain2 <- get_chain(proposal_sd = 0.01, init = 0.7)
chain3 <- get_chain(proposal_sd = 0.01, init = 0.95)
chain4 <- get_chain(proposal_sd = 0.01, init = 0.01)
chain1 <- data.frame(p = chain1, iter = 1:n_iter, chain = "chain 1")
chain2 <- data.frame(p = chain2, iter = 1:n_iter, chain = "chain 2")
chain3 <- data.frame(p = chain3, iter = 1:n_iter, chain = "chain 3")
chain4 <- data.frame(p = chain4, iter = 1:n_iter, chain = "chain 4")

my_df <- as.data.frame(rbind(chain1, chain2, chain3, chain4))

my_df %>%
  ggplot(aes(x = iter, y = p, group = chain, color = chain)) +
  geom_line(alpha = 0.4) +
  theme_qfc() +
  ylim(0, 1) +
  annotate("rect", xmin = -Inf, xmax = 5000, ymin = -Inf, ymax = Inf, alpha = 0.2, fill = "gray") +
  xlab("iteration of algorithm") +
  ylab("value of p") +
  theme(
    legend.position = c(0.8, 0.8), text = element_text(size = 20),
    legend.background = element_rect(fill = "white")
  )
```

## Running multiple chains each with different initialization values

```{R, echo = F}
my_df %>%
  ggplot(aes(x = iter, y = p, group = chain, color = chain)) +
  geom_line(alpha = 0.4) +
  theme_qfc() +
  ylim(0, 1) +
  annotate("rect", xmin = -Inf, xmax = 5000, ymin = -Inf, ymax = Inf, alpha = 0.2, fill = "gray") +
  xlab("iteration of algorithm") +
  ylab("value of p") +
  theme(
    legend.position = c(0.8, 0.8), text = element_text(size = 20),
    legend.background = element_rect(fill = "white")
  ) +
  geom_hline(aes(yintercept = sum(y) / n),
    color = "darkorange2",
    lty = 1, lwd = 2, alpha = 0.6
  )
```

## Software that can run MCMC for you

-   BUGS/JAGS
-   NIMBLE (de Valpine et al. 2017)
-   MCMCpack
-   LaplacesDemon
-   Stan (what we will use in this class)
-   Many others

These tools implement many different MCMC routines Ecologists can (mostly) treat these tools as black boxes

[e.g., see Gelman et al. 2003; Green et al. 2020; Kery and Schaub 2013]{.footerRight}

## Hamiltonian Monte Carlo (HMC)

#### The following draws heavily from Chapter 9 in McElreath 2023

::: {style="font-size: 75%;"}
*It appears to be a quite general principle that, wherever there is a randomized way of doing something, then there is a nonrandomized way that delivers better performance but requires more thought*-E.T. Jaynes

-   No free lunch
:::

![](images/jaynes.png){fig-align="center" width="200" height="300"}

[Wikipedia](https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes){.footerRight}

## Hamiltonian Monte Carlo (HMC)

-   HMC is much more computationally demanding than MH, but its proposals are much more efficient
-   HMC requires fewer samples to map out the posterior distribution

**The bottom line**: <br> You need less computer time in total, even though each sample requires more time than simpler algorithms

-   Really outshines other algorithms for models with 10s to 1000s of parameters

-   Don't need to know everything (lots of math in references)

[see McElreath 2023 chapter 9]{.footerRight}

## HMC: understanding some concepts

-   Leverages tools from [Hamiltonian dynamics](https://en.wikipedia.org/wiki/Hamiltonian_mechanics) and [differential geometry](https://en.wikipedia.org/wiki/Differential_geometry) to generate better proposals
-   Uses information on the gradient (curvature) of the log-posterior
-   HMC creates a $\theta^{\prime}$ that is uncorrelated with $\theta_{t}$ and which has a high probability of being accepted
    -   Does this using a physics simulation and pretends parameters are tiny, frictionless particles
-   See also Neal (2011) [MCMC using Hamiltonian Dynamics](https://arxiv.org/pdf/1206.1901.pdf%20http://arxiv.org/abs/1206.1901.pdf)

## HMC hockey puck analogy

-   Imagine you have a standard two parameter model
    -   This creates a posterior shaped like a bowl
-   In this case, HMC is like a hockey puck launched in random directions around an ice rink shaped like a bowl (the posterior distribution)

[Go to MCMC interactive gallery](https://chi-feng.github.io/mcmc-demo/app.html)

## Stan does the HMC sneakery for you

-   In the Stan language, stochastic models are described by specifying stochastic or deterministic relationships between quantities such as parameters and data

-   Stan (software) constructs the log-posterior for you if you specify prior(s) and likelihood(s)

-   Applies HMC to your problem, reports chains, log-posterior, and diagnostics

![](images/stan.png){style="width:300px; height:125px; position: absolute; margin-top:4%; right: 3%;"}

## Summary and outlook

Posteriors are difficult to approximate:

1.  Analytical solutions (beta-binomial)
2.  Brute force (grid search approximations)
3.  MCMC (draw samples from the posterior)

We have laid the ground work, and now we get to play and fit Bayesian models in Stan

![](images/caveman.png){style="width:400px; height:400; position: absolute; margin-top: -2%; right: 3%;"}

## References

::: {style="font-size:25px"}
1.  Gelman et al. 2003. Bayesian Data Analysis. Appendix C.

2.  Green et al. 2020. Introduction to Bayesian Methods in Ecology and Natural Resources. Appendix B.

3.  Kery and Schaub. 2012. Bayesian Population Analysis using WinBUGS.

4.  McElreath 2023. Statistical Rethinking. Second Edition, Chapters 2 and 9.

5.  Neal 2011. MCMC using Hamiltonian Dynamics. *In*: Handbook of Markov Chain Monte Carlo.

6.  Robert and Casella 2010. Introduction to Monte Carlo methods with R.

Useful web link:

[MCMC visualization app](https://chi-feng.github.io/mcmc-demo/app.html)

Michael Betancourt [seminar](https://www.youtube.com/watch?v=9ykCU2-W_8Y) on using Hamiltonian Monte Carlo for Bayesian inference
:::
